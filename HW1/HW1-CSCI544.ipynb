{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai Kumar\n",
      "[nltk_data]     Peddholla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Kumar Peddholla\\AppData\\Local\\Temp\\ipykernel_12580\\636854563.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_table('amazon_reviews_us_Beauty_v1_00.tsv', on_bad_lines='skip');\n"
     ]
    }
   ],
   "source": [
    " dataframe = pd.read_table('amazon_reviews_us_Beauty_v1_00.tsv', on_bad_lines='skip'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n"
     ]
    }
   ],
   "source": [
    "print(list(dataframe)) \n",
    "df = dataframe[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "4605644            1  The tube is tiny, about half the size of a reg...\n",
      "2455833            1  I bought this as a travel kit, used it once an...\n",
      "2920492            1  I DO NOT RECOMMEND<br />I WANT MY MONEY BACK<b...\n",
      "941661             1  My boyfriend got the PERFUME for me, for Chris...\n",
      "648648             1                               Does not stay in .-.\n",
      "...              ...                                                ...\n",
      "2812603            3  I absolutely love this flat iron. It works bet...\n",
      "3406498            3  These mitts were  purchased for my hubby for C...\n",
      "3445375            3  I have thick, frizz prone, color treated hair ...\n",
      "2570781            3                                              Great\n",
      "3646551            3  This is my fourth order of this paper. Its per...\n",
      "\n",
      "[60000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "class1 = df.loc[df['star_rating'].isin([1,2])]\n",
    "class2 = df.loc[df['star_rating'].isin([3])]\n",
    "class3 = df.loc[df['star_rating'].isin([4,5])]\n",
    "\n",
    "class1 = class1.sample(n=20000)\n",
    "class2 = class2.sample(n=20000)\n",
    "class3 = class3.sample(n=20000)\n",
    "\n",
    "class1['star_rating'] = class1['star_rating'].apply(lambda x: 1)\n",
    "class2['star_rating'] = class2['star_rating'].apply(lambda x: 2)\n",
    "class3['star_rating'] = class3['star_rating'].apply(lambda x: 3)\n",
    "\n",
    "sample_data = pd.concat([class1, class2, class3], axis=0)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287.49901660110675,285.05855\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# https://stackoverflow.com/questions/45999415/removing-html-tags-in-pandas\n",
    "# https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "\n",
    "average_length_before_cleaning = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: str(x).lower())\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: str(x).strip())\n",
    "sample_data['review_body'] = sample_data['review_body'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].str.replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: contractions.fix(str(x)))\n",
    "\n",
    "average_length_after_cleaning = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "print(str(average_length_before_cleaning)+\",\"+str(average_length_after_cleaning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai Kumar\n",
      "[nltk_data]     Peddholla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "4605644            1  tube tiny half size regular chapstick color su...\n",
      "2455833            1  bought travel kit used threw rather pack bring...\n",
      "2920492            1  recommendi want money backit cut whole skin wa...\n",
      "941661             1  boyfriend got perfume christmas ran ordered th...\n",
      "648648             1                                               stay\n",
      "...              ...                                                ...\n",
      "2812603            3  absolutely love flat iron works better chi str...\n",
      "3406498            3  mitts purchased hubby christmas couples massag...\n",
      "3445375            3  thick frizz prone color treated hair natural w...\n",
      "2570781            3                                              great\n",
      "3646551            3  fourth order paper perfect gets job done trans...\n",
      "\n",
      "[60000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "average_length_before_preprocessing = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# https://stackoverflow.com/questions/47557563/lemmatization-of-all-pandas-cells\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lemmatize_words)\n",
    "print(sample_data)\n",
    "\n",
    "\n",
    "average_length_after_preprocessing = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "\n",
    "print(str(average_length_before_preprocessing)+\",\"+str(average_length_after_preprocessing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# https://stackoverflow.com/questions/37593293/how-to-get-tfidf-with-pandas-dataframe\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_vector = vectorizer.fit_transform(sample_data['review_body'])\n",
    "print(feature_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037473976405274,0.6447628458498024,0.6235814120176801\n",
      "0.49827265479670474,0.47831632653061223,0.4880905896134322\n",
      "0.6727133367399081,0.6530257936507936,0.6627233828341303\n",
      "0.5924648705156038,0.5931666666666666,0.5924727788932864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "# splitting the data into train and test\n",
    "rating_data = np.array(sample_data['star_rating'])\n",
    "rating_data = rating_data.astype('int')\n",
    "review_train_data,review_test_data,rating_train_data,rating_test_data = train_test_split(feature_vector, rating_data, test_size=0.20)\n",
    "\n",
    "\n",
    "# https://python-course.eu/machine-learning/perceptron-class-in-sklearn.php\n",
    "perceptron = Perceptron(random_state=7,max_iter=1000,tol=0.001)\n",
    "\n",
    "perceptron_model = perceptron.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = perceptron_model.predict(review_test_data)\n",
    "\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names, output_dict=True)\n",
    "\n",
    "print(str(report['Rating 1']['precision'])+\",\"+str(report['Rating 1']['recall'])+\",\"+str(report['Rating 1']['f1-score']))\n",
    "print(str(report['Rating 2']['precision'])+\",\"+str(report['Rating 2']['recall'])+\",\"+str(report['Rating 2']['f1-score']))\n",
    "print(str(report['Rating 3']['precision'])+\",\"+str(report['Rating 3']['recall'])+\",\"+str(report['Rating 3']['f1-score']))\n",
    "print(str(report['weighted avg']['precision'])+\",\"+str(report['weighted avg']['recall'])+\",\"+str(report['weighted avg']['f1-score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6907216494845361,0.678606719367589,0.6846105919003115\n",
      "0.5698040164529398,0.6007653061224489,0.5848752017881536\n",
      "0.7562982005141388,0.7296626984126984,0.7427417318858874\n",
      "0.6732556105068278,0.6703333333333333,0.6715624274988268\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python\n",
    "\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "svm_model = svm_classifier.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = svm_model.predict(review_test_data)\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names,output_dict=True)\n",
    "\n",
    "\n",
    "# print(report)\n",
    "\n",
    "print(str(report['Rating 1']['precision'])+\",\"+str(report['Rating 1']['recall'])+\",\"+str(report['Rating 1']['f1-score']))\n",
    "print(str(report['Rating 2']['precision'])+\",\"+str(report['Rating 2']['recall'])+\",\"+str(report['Rating 2']['f1-score']))\n",
    "print(str(report['Rating 3']['precision'])+\",\"+str(report['Rating 3']['recall'])+\",\"+str(report['Rating 3']['f1-score']))\n",
    "print(str(report['weighted avg']['precision'])+\",\"+str(report['weighted avg']['recall'])+\",\"+str(report['weighted avg']['f1-score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6951128752170678,0.692193675889328,0.6936502042331971\n",
      "0.5834601725012684,0.5867346938775511,0.5850928516916816\n",
      "0.7514278619319593,0.7504960317460317,0.7509616577739173\n",
      "0.6775614945327769,0.6773333333333333,0.6774447841259841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "\n",
    "logisticRegr = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logisticRegression_model = logisticRegr.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = logisticRegression_model.predict(review_test_data)\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names,output_dict=True)\n",
    "\n",
    "# print(report)\n",
    "\n",
    "print(str(report['Rating 1']['precision'])+\",\"+str(report['Rating 1']['recall'])+\",\"+str(report['Rating 1']['f1-score']))\n",
    "print(str(report['Rating 2']['precision'])+\",\"+str(report['Rating 2']['recall'])+\",\"+str(report['Rating 2']['f1-score']))\n",
    "print(str(report['Rating 3']['precision'])+\",\"+str(report['Rating 3']['recall'])+\",\"+str(report['Rating 3']['f1-score']))\n",
    "print(str(report['weighted avg']['precision'])+\",\"+str(report['weighted avg']['recall'])+\",\"+str(report['weighted avg']['f1-score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6969368392518298,0.6351284584980237,0.6645986816595579\n",
      "0.5412556053811659,0.6158163265306122,0.5761336515513126\n",
      "0.7289015840041547,0.6961805555555556,0.7121654192566281\n",
      "0.6568211237575274,0.6493333333333333,0.6516825289901468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "naiveBayes_model = MultinomialNB(alpha=0.85).fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = naiveBayes_model.predict(review_test_data)\n",
    "\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names, output_dict=True)\n",
    "\n",
    "print(str(report['Rating 1']['precision'])+\",\"+str(report['Rating 1']['recall'])+\",\"+str(report['Rating 1']['f1-score']))\n",
    "print(str(report['Rating 2']['precision'])+\",\"+str(report['Rating 2']['recall'])+\",\"+str(report['Rating 2']['f1-score']))\n",
    "print(str(report['Rating 3']['precision'])+\",\"+str(report['Rating 3']['recall'])+\",\"+str(report['Rating 3']['f1-score']))\n",
    "print(str(report['weighted avg']['precision'])+\",\"+str(report['weighted avg']['recall'])+\",\"+str(report['weighted avg']['f1-score']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
