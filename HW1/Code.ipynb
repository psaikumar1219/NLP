{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai Kumar\n",
      "[nltk_data]     Peddholla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Kumar Peddholla\\AppData\\Local\\Temp\\ipykernel_5440\\636854563.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_table('amazon_reviews_us_Beauty_v1_00.tsv', on_bad_lines='skip');\n"
     ]
    }
   ],
   "source": [
    " dataframe = pd.read_table('amazon_reviews_us_Beauty_v1_00.tsv', on_bad_lines='skip'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n"
     ]
    }
   ],
   "source": [
    "print(list(dataframe)) \n",
    "df = dataframe[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "1769386            1  Ordered two; one for me, and one for my wife. ...\n",
      "3430167            1  These hair claws are absolutely no better (or ...\n",
      "4882575            1  Tried this a few years ago. Unfortunately if y...\n",
      "3568322            1  I have had this toothbrush for 3 years and thi...\n",
      "3125499            1  These sounded good but were not.  They made th...\n",
      "...              ...                                                ...\n",
      "3368356            3  These towelettes do an excellent job of removi...\n",
      "234398             3  Warmth is a great product for every season of ...\n",
      "1606247            3  Smells great but not any better than the oil a...\n",
      "3478897            3  Very big bottle that lasts me for months.  Hel...\n",
      "3108499            3  MY FRIENDS LET ME BUY IT FOR HER<br />AND WHEN...\n",
      "\n",
      "[60000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "class1 = df.loc[df['star_rating'].isin([1,2])]\n",
    "class2 = df.loc[df['star_rating'].isin([3])]\n",
    "class3 = df.loc[df['star_rating'].isin([4,5])]\n",
    "\n",
    "class1 = class1.sample(n=20000)\n",
    "class2 = class2.sample(n=20000)\n",
    "class3 = class3.sample(n=20000)\n",
    "\n",
    "class1['star_rating'] = class1['star_rating'].apply(lambda x: 1)\n",
    "class2['star_rating'] = class2['star_rating'].apply(lambda x: 2)\n",
    "class3['star_rating'] = class3['star_rating'].apply(lambda x: 3)\n",
    "\n",
    "sample_data = pd.concat([class1, class2, class3], axis=0)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287.0545693951364,284.68385\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# https://stackoverflow.com/questions/45999415/removing-html-tags-in-pandas\n",
    "# https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe\n",
    "\n",
    "average_length_before_cleaning = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: str(x).lower())\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: str(x).strip())\n",
    "sample_data['review_body'] = sample_data['review_body'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].str.replace(r'[^a-zA-Z]',' ',regex=True)\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: contractions.fix(str(x)))\n",
    "\n",
    "average_length_after_cleaning = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "print(str(average_length_before_cleaning)+\",\"+str(average_length_after_cleaning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai Kumar\n",
      "[nltk_data]     Peddholla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "1769386            1  ordered two one one wife color use marker pen ...\n",
      "3430167            1  hair claws absolutely better worse waythey wor...\n",
      "4882575            1  tried years ago unfortunately mildly active we...\n",
      "3568322            1  toothbrush years morning finally died brushed ...\n",
      "3125499            1  sounded good made bath water greasy ugh left t...\n",
      "...              ...                                                ...\n",
      "3368356            3  towelettes excellent job removing make whether...\n",
      "234398             3  warmth great product every season make wear we...\n",
      "1606247            3                smells great better oil oil cheaper\n",
      "3478897            3  big bottle lasts months helped hair feel nice ...\n",
      "3108499            3          friends let buy herand got like naturural\n",
      "\n",
      "[60000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "average_length_before_preprocessing = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "1769386            1  order two one one wife color use marker pen te...\n",
      "3430167            1  hair claw absolutely good bad waythey work app...\n",
      "4882575            1  try year ago unfortunately mildly active wear ...\n",
      "3568322            1  toothbrush year morning finally die brush twic...\n",
      "3125499            1  sound good make bath water greasy ugh leave tu...\n",
      "...              ...                                                ...\n",
      "3368356            3  towelettes excellent job remove make whether l...\n",
      "234398             3  warmth great product every season make wear we...\n",
      "1606247            3                     smell great good oil oil cheap\n",
      "3478897            3  big bottle last month help hair feel nice soft...\n",
      "3108499            3           friend let buy herand get like naturural\n",
      "\n",
      "[60000 rows x 2 columns]\n",
      "284.68385,157.93593333333334\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# https://stackoverflow.com/questions/47557563/lemmatization-of-all-pandas-cells\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "    words = [lemmatizer.lemmatize(word,pos='a') for word in words]\n",
    "    words = [lemmatizer.lemmatize(word,pos='n') for word in words]\n",
    "    words = [lemmatizer.lemmatize(word,pos='r') for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "sample_data['review_body'] = sample_data['review_body'].apply(lemmatize_words)\n",
    "print(sample_data)\n",
    "\n",
    "\n",
    "average_length_after_preprocessing = sample_data['review_body'].str.len().mean()\n",
    "\n",
    "\n",
    "print(str(average_length_before_preprocessing)+\",\"+str(average_length_after_preprocessing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3183)\t0.1661193897967145\n",
      "  (0, 1846)\t0.32528702680852406\n",
      "  (0, 14084)\t0.19487302484500882\n",
      "  (0, 1240)\t0.1760571939080441\n",
      "  (0, 5626)\t0.33640280180082055\n",
      "  (0, 3102)\t0.23793841931135845\n",
      "  (0, 14000)\t0.14128720647438744\n",
      "  (0, 11226)\t0.18540427056707065\n",
      "  (0, 1161)\t0.2409944107852801\n",
      "  (0, 24228)\t0.196355264790905\n",
      "  (0, 17804)\t0.31340545131461117\n",
      "  (0, 14579)\t0.35308307980160436\n",
      "  (0, 26152)\t0.26030635075912834\n",
      "  (0, 4667)\t0.1480498615062033\n",
      "  (0, 27182)\t0.23133645789227278\n",
      "  (0, 16860)\t0.2364721310256525\n",
      "  (0, 25473)\t0.1729098768311362\n",
      "  (0, 17027)\t0.15800876412698858\n",
      "  (1, 15434)\t0.17643417993347774\n",
      "  (1, 21240)\t0.5106791632127007\n",
      "  (1, 4053)\t0.19202397384801748\n",
      "  (1, 1204)\t0.298419556660133\n",
      "  (1, 27399)\t0.11956501386181118\n",
      "  (1, 26900)\t0.5106791632127007\n",
      "  (1, 1875)\t0.17493038795413465\n",
      "  :\t:\n",
      "  (59997, 4053)\t0.39177406241699975\n",
      "  (59997, 10167)\t0.22963505534832307\n",
      "  (59998, 5027)\t0.5006304915002902\n",
      "  (59998, 5811)\t0.4930722317532508\n",
      "  (59998, 23436)\t0.22643431625458987\n",
      "  (59998, 26891)\t0.2949578614346329\n",
      "  (59998, 11019)\t0.204447478802872\n",
      "  (59998, 2918)\t0.19426293481641582\n",
      "  (59998, 8769)\t0.1670525810338128\n",
      "  (59998, 13444)\t0.17874784592729187\n",
      "  (59998, 15472)\t0.19634489338773847\n",
      "  (59998, 2491)\t0.21661448804098\n",
      "  (59998, 16152)\t0.1808296070295591\n",
      "  (59998, 22585)\t0.20616945281387228\n",
      "  (59998, 22408)\t0.155228659248715\n",
      "  (59998, 10167)\t0.12221069567874751\n",
      "  (59998, 10618)\t0.13656280074164234\n",
      "  (59998, 26152)\t0.10110833788889197\n",
      "  (59999, 15949)\t0.6240422630024683\n",
      "  (59999, 11045)\t0.6240422630024683\n",
      "  (59999, 9491)\t0.28810418576665725\n",
      "  (59999, 3409)\t0.15414738706349707\n",
      "  (59999, 13645)\t0.2760545563130503\n",
      "  (59999, 13749)\t0.13543141083998453\n",
      "  (59999, 9876)\t0.1408164907794546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# https://stackoverflow.com/questions/37593293/how-to-get-tfidf-with-pandas-dataframe\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_vector = vectorizer.fit_transform(sample_data['review_body'])\n",
    "print(feature_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801909307875895,0.08345752608047691,0.15118110236220475\n",
      "0.3582069699388135,0.6805660854182461,0.4693681917211328\n",
      "0.49372384937238495,0.49937764500871296,0.4965346534653466\n",
      "0.5524333797070924,0.4195833333333333,0.371710396310088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "# splitting the data into train and test\n",
    "rating_data = np.array(sample_data['star_rating'])\n",
    "rating_data = rating_data.astype('int')\n",
    "review_train_data,review_test_data,rating_train_data,rating_test_data = train_test_split(feature_vector, rating_data, test_size=0.20)\n",
    "\n",
    "\n",
    "# https://python-course.eu/machine-learning/perceptron-class-in-sklearn.php\n",
    "perceptron = Perceptron(random_state=42,max_iter=10000,alpha=0.001,penalty='l1')\n",
    "\n",
    "perceptron_model = perceptron.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = perceptron_model.predict(review_test_data)\n",
    "\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names, output_dict=True)\n",
    "\n",
    "rating1_data = report['Rating 1']\n",
    "rating2_data = report['Rating 2']\n",
    "rating3_data = report['Rating 3']\n",
    "weighted_avg_data = report['weighted avg']\n",
    "\n",
    "\n",
    "print(str(rating1_data['precision'])+\",\"+str(rating1_data['recall'])+\",\"+str(rating1_data['f1-score']))\n",
    "print(str(rating2_data['precision'])+\",\"+str(rating2_data['recall'])+\",\"+str(rating2_data['f1-score']))\n",
    "print(str(rating3_data['precision'])+\",\"+str(rating3_data['recall'])+\",\"+str(rating3_data['f1-score']))\n",
    "print(str(weighted_avg_data['precision'])+\",\"+str(weighted_avg_data['recall'])+\",\"+str(weighted_avg_data['f1-score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756422924901185,0.6851202404809619,0.6803482587064676\n",
      "0.5825147347740668,0.587565023532326,0.5850289801455174\n",
      "0.7507731958762887,0.7335683706874843,0.742071073748567\n",
      "0.6691746980606835,0.6683333333333333,0.6687063729549361\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python\n",
    "\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "svm_model = svm_classifier.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = svm_model.predict(review_test_data)\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names,output_dict=True)\n",
    "\n",
    "\n",
    "# print(report)\n",
    "\n",
    "rating1_data = report['Rating 1']\n",
    "rating2_data = report['Rating 2']\n",
    "rating3_data = report['Rating 3']\n",
    "weighted_avg_data = report['weighted avg']\n",
    "\n",
    "\n",
    "print(str(rating1_data['precision'])+\",\"+str(rating1_data['recall'])+\",\"+str(rating1_data['f1-score']))\n",
    "print(str(rating2_data['precision'])+\",\"+str(rating2_data['recall'])+\",\"+str(rating2_data['f1-score']))\n",
    "print(str(rating3_data['precision'])+\",\"+str(rating3_data['recall'])+\",\"+str(rating3_data['f1-score']))\n",
    "print(str(weighted_avg_data['precision'])+\",\"+str(weighted_avg_data['recall'])+\",\"+str(weighted_avg_data['f1-score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687221396731055,0.6951402805611222,0.6911581569115816\n",
      "0.5899823810722375,0.5806291800842209,0.5852684144818976\n",
      "0.7402857859112559,0.7436414001510955,0.741959798994975\n",
      "0.6720684619993826,0.6726666666666666,0.672346192788625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "\n",
    "logisticRegr = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logisticRegression_model = logisticRegr.fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = logisticRegression_model.predict(review_test_data)\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names,output_dict=True)\n",
    "\n",
    "# print(report)\n",
    "\n",
    "rating1_data = report['Rating 1']\n",
    "rating2_data = report['Rating 2']\n",
    "rating3_data = report['Rating 3']\n",
    "weighted_avg_data = report['weighted avg']\n",
    "\n",
    "\n",
    "print(str(rating1_data['precision'])+\",\"+str(rating1_data['recall'])+\",\"+str(rating1_data['f1-score']))\n",
    "print(str(rating2_data['precision'])+\",\"+str(rating2_data['recall'])+\",\"+str(rating2_data['f1-score']))\n",
    "print(str(rating3_data['precision'])+\",\"+str(rating3_data['recall'])+\",\"+str(rating3_data['f1-score']))\n",
    "print(str(weighted_avg_data['precision'])+\",\"+str(weighted_avg_data['recall'])+\",\"+str(weighted_avg_data['f1-score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6840249525359371,0.6317635270541082,0.6568563615054043\n",
      "0.5577742876374243,0.6158038147138964,0.5853543677890276\n",
      "0.7261410788381742,0.7051120624527827,0.7154720838124441\n",
      "0.6554890528151778,0.6506666666666666,0.6521988185594244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "naiveBayes_model = MultinomialNB(alpha=0.85).fit(review_train_data,rating_train_data)\n",
    "rating_predicted_data = naiveBayes_model.predict(review_test_data)\n",
    "\n",
    "\n",
    "target_names = ['Rating 1', 'Rating 2', 'Rating 3']\n",
    "report = classification_report(rating_test_data, rating_predicted_data, target_names=target_names, output_dict=True)\n",
    "\n",
    "rating1_data = report['Rating 1']\n",
    "rating2_data = report['Rating 2']\n",
    "rating3_data = report['Rating 3']\n",
    "weighted_avg_data = report['weighted avg']\n",
    "\n",
    "\n",
    "print(str(rating1_data['precision'])+\",\"+str(rating1_data['recall'])+\",\"+str(rating1_data['f1-score']))\n",
    "print(str(rating2_data['precision'])+\",\"+str(rating2_data['recall'])+\",\"+str(rating2_data['f1-score']))\n",
    "print(str(rating3_data['precision'])+\",\"+str(rating3_data['recall'])+\",\"+str(rating3_data['f1-score']))\n",
    "print(str(weighted_avg_data['precision'])+\",\"+str(weighted_avg_data['recall'])+\",\"+str(weighted_avg_data['f1-score']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
