{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 544 - HW3 - USC ID: 5978435849"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dependencies:\n",
    " Python version: 3.11.0\n",
    " Conda version: 23.1.0\n",
    " pytorch version: 1.13.1\n",
    " \n",
    " References:\n",
    " 1. https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    " 2. https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    " 3. https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    " 4. https://youtu.be/WEV61GmmPrk\n",
    " 5. https://youtu.be/0_PgWWmauHk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as dwnld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Kumar Peddholla\\AppData\\Local\\Temp\\ipykernel_9816\\99976983.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_table('data.tsv', on_bad_lines='skip');\n"
     ]
    }
   ],
   "source": [
    "# dataframe = pd.read_table('amazon_reviews_us_Beauty_v1_00.tsv', on_bad_lines='skip');\n",
    "dataframe = pd.read_table('data.tsv', on_bad_lines='skip');\n",
    "# print(list(dataframe))\n",
    "df = dataframe[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating                                        review_body\n",
      "243524             1                            DON'T WASTE YOUR MONEY.\n",
      "598830             1  At first glance this was an awesome product un...\n",
      "3681814            1  I have used this product for about a year and ...\n",
      "3419093            1  I received my order today and I am a bit conce...\n",
      "421958             1  I've only had it since April, and it's already...\n",
      "...              ...                                                ...\n",
      "1685159            3  ... so relaxing to take bath with this stuff ....\n",
      "3818342            3  It is the best size pillow for laying in tub. ...\n",
      "1892285            3                       Leaves my skin silky smooth.\n",
      "2174487            3  mainly used in conjunction with light moisturi...\n",
      "4606435            3  I have been looking for something like this fo...\n",
      "\n",
      "[60000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generating random sample data, from each class\n",
    "class1 = df.loc[df['star_rating'].isin([1,2])]\n",
    "class2 = df.loc[df['star_rating'].isin([3])]\n",
    "class3 = df.loc[df['star_rating'].isin([4,5])]\n",
    "\n",
    "class1 = class1.sample(n=20000)\n",
    "class2 = class2.sample(n=20000)\n",
    "class3 = class3.sample(n=20000)\n",
    "\n",
    "class1['star_rating'] = class1['star_rating'].apply(lambda x: 1)\n",
    "class2['star_rating'] = class2['star_rating'].apply(lambda x: 2)\n",
    "class3['star_rating'] = class3['star_rating'].apply(lambda x: 3)\n",
    "\n",
    "sample_data = pd.concat([class1, class2, class3], axis=0)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Loading google word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google_model = dwnld.load('word2vec-google-news-300')\n",
    "# Used below while testing to save loading time\n",
    "# w2v_google_model.save('./w2v_google_model.model')\n",
    "# w2v_google_model = KeyedVectors.load('w2v_google_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king~queen: 0.6510957\n",
      "man~woman: 0.76640123\n",
      "excellent~outstanding: 0.5567486\n",
      "sad~unhappy: 0.41572243\n",
      "fast~quick: 0.57016057\n"
     ]
    }
   ],
   "source": [
    "# Computing simillarities between words\n",
    "print(\"king~queen:\",w2v_google_model.similarity(\"king\", \"queen\"))\n",
    "print(\"man~woman:\",w2v_google_model.similarity(\"man\", \"woman\"))\n",
    "print(\"excellent~outstanding:\",w2v_google_model.similarity(\"excellent\", \"outstanding\"))\n",
    "print(\"sad~unhappy:\",w2v_google_model.similarity(\"sad\", \"unhappy\"))\n",
    "print(\"fast~quick:\",w2v_google_model.similarity(\"fast\", \"quick\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king->queen::man-> [('queens', 0.595018744468689), ('monarch', 0.5815044641494751), ('kings', 0.5612993240356445), ('royal', 0.5204525589942932), ('princess', 0.5191516876220703), ('princes', 0.5086392164230347), ('NYC_anglophiles_aflutter', 0.5057314038276672), ('Queen_Consort', 0.49256712198257446), ('Queen', 0.4822567403316498), ('royals', 0.4781742990016937)]\n",
      "excellent->outstanding::sad-> [('oustanding', 0.6630989909172058), ('exceptional', 0.6203196048736572), ('superb', 0.5520898103713989), ('exemplary', 0.5066716074943542), ('terrific', 0.5014314651489258), ('Excellent', 0.4898416996002197), ('impeccable', 0.47491276264190674), ('superior', 0.4728458523750305), ('superlative', 0.47127777338027954), ('Outstanding', 0.46214401721954346)]\n"
     ]
    }
   ],
   "source": [
    "# Computing simillarities between words\n",
    "print(\"king->queen::man->\",w2v_google_model.most_similar(positive=['king','queen'],negative=['man']))\n",
    "print(\"excellent->outstanding::sad->\",w2v_google_model.most_similar(positive=['excellent','outstanding'],negative=['sad']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Word2Vec model using amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai Kumar Peddholla\\AppData\\Local\\Temp\\ipykernel_9816\\839809871.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  reviews = sample_data['review_body'].str.lower().str.replace('[^\\w\\s]','').str.split().tolist()\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for Word2Vec model\n",
    "vector_size = 300 # dimensionality of the word vectors\n",
    "window_size = 13 # size of the context window\n",
    "min_count = 9 # minimum frequency of a word to be included in the vocabulary\n",
    "\n",
    "\n",
    "# Convert the reviews column to a list of sentences.lowercased the reviews and removed all non-alphanumeric characters \n",
    "reviews = sample_data['review_body'].str.lower().str.replace('[^\\w\\s]','').str.split().tolist()\n",
    "\n",
    "# The below code filters out any invalid values (e.g. floats) from each sub-list using a list comprehension\n",
    "filtered_reviews = []\n",
    "for review in reviews:\n",
    "    if isinstance(review, list):\n",
    "        cur_list = []\n",
    "        for word in review:\n",
    "            if isinstance(word, str):\n",
    "                cur_list.append(word)\n",
    "        filtered_reviews.append(cur_list)\n",
    "    else:\n",
    "        filtered_reviews.append([])\n",
    "\n",
    "reviews = filtered_reviews\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(reviews, vector_size=vector_size, window=window_size, min_count=min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king~queen: 0.5034378\n",
      "man~woman: 0.7376918\n",
      "excellent~outstanding: 0.7757071\n",
      "sad~unhappy: 0.55369514\n",
      "fast~quick: 0.7835034\n"
     ]
    }
   ],
   "source": [
    "# Computing simillarities between words\n",
    "print(\"king~queen:\",model.wv.similarity(\"king\", \"queen\"))\n",
    "print(\"man~woman:\",model.wv.similarity(\"man\", \"woman\"))\n",
    "print(\"excellent~outstanding:\",model.wv.similarity(\"excellent\", \"outstanding\"))\n",
    "print(\"sad~unhappy:\",model.wv.similarity(\"sad\", \"unhappy\"))\n",
    "print(\"fast~quick:\",model.wv.similarity(\"fast\", \"quick\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king->queen::man-> [('francisco', 0.7195603251457214), ('san', 0.6869110465049744), ('buckthorn', 0.6863482594490051), ('arbonne', 0.6852318644523621), ('palmitate', 0.6786541938781738), ('origin', 0.6753250360488892), ('avalon', 0.674170196056366), ('resurfacing', 0.6731386184692383), ('botanical', 0.673132061958313), ('cell', 0.6716758012771606)]\n",
      "excellent->outstanding::sad-> [('superb', 0.7255911231040955), ('inferior', 0.605215311050415), ('basic', 0.6045570969581604), ('exceptional', 0.6037482023239136), ('adequate', 0.588214099407196), ('interesting', 0.5643987655639648), ('providing', 0.5642382502555847), ('equal', 0.5430349111557007), ('includes', 0.5429861545562744), ('superior', 0.524884045124054)]\n"
     ]
    }
   ],
   "source": [
    "# Finding most suitable words based on analogy. Since the dataset is different simmilar words would \n",
    "# be different and also their probabilities\n",
    "print(\"king->queen::man->\",model.wv.most_similar(positive=['king','queen'],negative=['man']))\n",
    "print(\"excellent->outstanding::sad->\",model.wv.most_similar(positive=['excellent','outstanding'],negative=['sad']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec\n",
    "models seems to encode semantic similarities between words better?\n",
    "\n",
    "Ans. From the comparison of the vectors generated by the custom model (amazon dataset) and the google model, it can be concluded that the google Word2Vec model appears to encode semantic similarities between words better.\n",
    "\n",
    "In particular, the similarity scores for the word pairs \"king~queen\" and \"man~woman\" are higher in the google model, indicating a better encoding of the semantic relationship between these pairs. Additionally, the similarity score for \"excellent~outstanding\" in the custom model is higher than the similarity score for \"excellent~outstanding\" in the google model, further suggesting that the custom model better captures semantic similarities between words particular to its domain.\n",
    "\n",
    "Overall, the pre-trained Word2Vec model seems to have learned more general and robust semantic relationships between words, while the custom model may be limited by the specific dataset it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Dataset into Train and test. [This split is consistent for training all the models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes classes from 1->0; 2->1; 3->2 to make it cinsistent in svm, percpetron and FNNs(since target values should start from 0)\n",
    "y = sample_data['star_rating'].map({1: 0, 2: 1, 3: 2})\n",
    "X = reviews\n",
    "X_train_main, X_test_main, y_train_main, y_test_main = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing average of word2vec features of all words for a review\n",
    "def get_word2vec(review):\n",
    "    words = review\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in w2v_google_model:\n",
    "            vectors.append(w2v_google_model[word])\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "X_train = np.array([get_word2vec(review) for review in X_train_main])\n",
    "X_test = np.array([get_word2vec(review) for review in X_test_main])\n",
    "y_train = y_train_main\n",
    "y_test = y_test_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Perceptron and SVM using word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron model\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred = perceptron.predict(X_test)\n",
    "word2vec_perceptron_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# SVM model\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "word2vec_svm_acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec perceptron accuracy:  0.4176666666666667\n",
      "Word2Vec svm accuracy:  0.6515833333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec perceptron accuracy: \",word2vec_perceptron_acc)\n",
    "print(\"Word2Vec svm accuracy: \",word2vec_svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Perceptron and SVM using Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer with desired parameters\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_temp = sample_data['review_body'].fillna('')\n",
    "# Fit and transform the reviews using the vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X_temp)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size=0.2)\n",
    "\n",
    "# Perceptron model\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred = perceptron.predict(X_test_tfidf)\n",
    "tfidf_perceptron_acc = accuracy_score(y_test_tfidf, y_pred)\n",
    "\n",
    "# SVM model\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "tfidf_svm_acc = accuracy_score(y_test_tfidf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-Idf perceptron accuracy:  0.6948333333333333\n",
      "Tf-Idf svm accuracy:  0.72525\n"
     ]
    }
   ],
   "source": [
    "print(\"Tf-Idf perceptron accuracy: \",tfidf_perceptron_acc)\n",
    "print(\"Tf-Idf svm accuracy: \",tfidf_svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you conclude from comparing performances for the models trained using the two different feature types?\n",
    "\n",
    "Ans. Based on the accuracy scores, it can be concluded that the models trained using the Tf-Idf feature type outperformed the models trained using the Word2Vec feature type.\n",
    "\n",
    "The perceptron and SVM models trained on Tf-Idf features achieved higher accuracy scores compared to those trained on Word2Vec features. The perceptron model achieved an accuracy of 0.69 with Tf-Idf features, while it achieved only 0.41 with Word2Vec features. Similarly, the SVM model achieved an accuracy of 0.72 with Tf-Idf features, while it achieved only 0.65 with Word2Vec features.\n",
    "\n",
    "This suggests that the Tf-Idf feature type is better suited for sentiment analysis tasks than the Word2Vec feature type. This may be because the Tf-Idf representation captures the frequency of words in each document, which can be indicative of sentiment, while the Word2Vec representation may not capture all aspects (since we are computing average some significant features might be diluted) of sentiment.\n",
    "\n",
    "However, the computation time for training Tf-Idf models is way more than the computation time for word2vec model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Simple FNN unit cell using average word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining FNN architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "input_size = 300 # size of Word2Vec vectors\n",
    "hidden_size1 = 100 # first layer\n",
    "hidden_size2 = 10 # second layer\n",
    "output_size = 3 # number of rating categories\n",
    "\n",
    "# Defining hyperparamters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.6190446615219116\n",
      "Epoch [20/50], Loss: 0.5855628848075867\n",
      "Epoch [30/50], Loss: 0.5434885621070862\n",
      "Epoch [40/50], Loss: 0.5073877573013306\n",
      "Epoch [50/50], Loss: 0.4767744839191437\n"
     ]
    }
   ],
   "source": [
    "# Creating input vectors for test and train\n",
    "X_train = np.array([get_word2vec(review) for review in X_train_main])\n",
    "X_test = np.array([get_word2vec(review) for review in X_test_main])\n",
    "\n",
    "# Initialize model\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training network\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n",
    "        batch_y = torch.LongTensor(y_train[i:i+batch_size].values)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print loss after every 10th epoch\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FNN (Average word2vec): 0.6546666666666666\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "with torch.no_grad():\n",
    "    # Convert testing set to tensors\n",
    "    test_X = torch.FloatTensor(X_test)\n",
    "    test_y = torch.LongTensor(y_test.to_numpy())\n",
    "    # Predict classes using testing set\n",
    "    outputs = model(test_X)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Calculate accuracy\n",
    "    total = test_y.size(0)\n",
    "    correct = (predicted == test_y).sum().item()\n",
    "    accuracy = correct / total\n",
    "    # Print accuracy\n",
    "    print('Accuracy on FNN (Average word2vec): {}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Simple FNN unit cell using first 10 words of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating word2vec for first 10 words by concatenation, setting default to 0's\n",
    "def get_word2vec_consider_first_10_words(reviews):\n",
    "    features = np.zeros((len(reviews), 3000))\n",
    "    for i, review in enumerate(reviews):\n",
    "        words = review[:10]\n",
    "        padded_vectors = np.zeros((10, 300))\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in w2v_google_model:\n",
    "                vectors.append(w2v_google_model[word])\n",
    "        if len(vectors) > 0:\n",
    "            padded_vectors[:len(vectors), :] = vectors\n",
    "            features[i] = padded_vectors.flatten()\n",
    "    return features\n",
    "\n",
    "X_train = get_word2vec_consider_first_10_words(X_train_main)\n",
    "X_test = get_word2vec_consider_first_10_words(X_test_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3000 # size of Word2Vec vectors\n",
    "hidden_size1 = 100 # first layer\n",
    "hidden_size2 = 10 # second layer\n",
    "output_size = 3 # number of rating categories\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.3133710026741028\n",
      "Epoch [20/50], Loss: 0.09786497801542282\n",
      "Epoch [30/50], Loss: 0.07481760531663895\n",
      "Epoch [40/50], Loss: 0.05096546560525894\n",
      "Epoch [50/50], Loss: 0.02985224686563015\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, using same architecture as defined earlier\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n",
    "        batch_y = torch.LongTensor(y_train[i:i+batch_size].values)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print loss after every 10th epoch\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FNN (considering first 10 words): 0.5158333333333334\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model on testing set\n",
    "with torch.no_grad():\n",
    "    # Convert testing set to tensors\n",
    "    test_X = torch.FloatTensor(X_test)\n",
    "    test_y = torch.LongTensor(y_test.to_numpy())\n",
    "    # Predict classes using testing set\n",
    "    outputs = model(test_X)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = test_y.size(0)\n",
    "    correct = (predicted == test_y).sum().item()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Print accuracy\n",
    "    print('Accuracy on FNN (considering first 10 words): {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you conclude by comparing accuracy values you obtain with those obtained in the “Simple Models” section.\n",
    "\n",
    "Ans. Comparing the accuracies obtained for the two cases of modeling a simple FNN using average word2vec features and using only the first 10 words of a review, it can be concluded that the FNN model using average word2vec features performed better than the one using only the first 10 words of a review. The accuracy of 0.65 for the FNN model with average word2vec features is higher than the accuracy of 0.515 for the FNN model with only the first 10 words of a review.\n",
    "\n",
    "When compared to the accuracies obtained for the SVM and perceptron models, it can be seen that the FNN models with average word2vec features and the SVM models with Tf-Idf features have similar accuracies, FNN is slightly better because a multi layer perceptron can learn better than a single perceptron (comples function can be learned by MLP). However, the FNN models with only the first 10 words of a review have lower accuracies. This suggests that using only the first few words of a review may not provide enough information for the models to accurately predict the sentiment of the review.\n",
    "\n",
    "Overall, it can be concluded that using more comprehensive feature representations, such as average word2vec or Tf-Idf features, can lead to better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Considering Simple RNN unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fixed review length\n",
    "max_review_length = 20\n",
    "\n",
    "# truncate reviews to length 20 if more, otherwise padding with '<PAD>'\n",
    "def truncate_reviews(reviews):\n",
    "    truncate_reviews = []\n",
    "    for review in reviews:\n",
    "        words = []\n",
    "        # Truncate or pad review to fixed length\n",
    "        if len(review) > max_review_length:\n",
    "            words = review[:max_review_length]\n",
    "        else:\n",
    "            words = review + ['<PAD>'] * (max_review_length - len(review))\n",
    "        truncate_reviews.append(words)\n",
    "    return truncate_reviews\n",
    "    \n",
    "# Get word to vec features of reviews. Concatenating all word2vec features of 20 words simillar to 4b\n",
    "def get_word2vec_consider_20_words(reviews):\n",
    "    features = np.zeros((len(reviews), 6000))\n",
    "    for i, review in enumerate(reviews):\n",
    "        words = review\n",
    "        padded_vectors = np.zeros((20, 300))\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in w2v_google_model:\n",
    "                vectors.append(w2v_google_model[word])\n",
    "        if len(vectors) > 0:\n",
    "            padded_vectors[:len(vectors), :] = vectors\n",
    "            features[i] = padded_vectors.flatten()\n",
    "    return features\n",
    "    \n",
    "processed_reviews_train = truncate_reviews(X_train_main)\n",
    "processed_reviews_test = truncate_reviews(X_test_main)\n",
    "X_train = get_word2vec_consider_20_words(processed_reviews_train)\n",
    "X_test = get_word2vec_consider_20_words(processed_reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RNN architecture\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "input_size = 6000 # size of word2vec features\n",
    "hidden_size = 20\n",
    "output_size = 3 # number of ratings\n",
    "\n",
    "# Defining hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.4484310746192932\n",
      "Epoch [20/50], Loss: 0.16086478531360626\n",
      "Epoch [30/50], Loss: 0.18855616450309753\n",
      "Epoch [40/50], Loss: 0.06513053923845291\n",
      "Epoch [50/50], Loss: 0.04815263673663139\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training network\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n",
    "        batch_y = torch.LongTensor(y_train[i:i+batch_size].values)\n",
    "        \n",
    "        batch_X = batch_X.reshape(batch_size, -1, input_size)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after every 10th epoch\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Simple RNN: 0.5170833333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    h = torch.zeros(1, len(X_test), hidden_size)\n",
    "    # Convert testing set to tensors\n",
    "    inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    inputs = inputs.reshape(12000, -1, input_size)\n",
    "    labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    # Predict classes using testing set\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted == labels).sum().item() / len(labels)\n",
    "    print('Accuracy on Simple RNN: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "\n",
    "Ans. Comparing the accuracies obtained for the FNN models and the Simple RNN model, it can be concluded that the FNN model using average word2vec features performed better than the Simple RNN model considering the first 20 words of a review.\n",
    "\n",
    "It is worth noting that the Simple RNN model may have been affected by the vanishing gradient problem, which can make it difficult for the model to learn long-term dependencies in the sequence data. In contrast, FNN models are more suitable for shallow learning tasks, where the sequence data may not contain long-term dependencies. Therefore, it is not surprising to see that the FNN model with average word2vec outperforms the Simple RNN model in this case.\n",
    "\n",
    "Comparing with 3b, since RNN is considering 20 words while FNN is considering only 10 words. RNN has slightly better accuracy than FNN.\n",
    "\n",
    "Overall, it can be concluded that the FNN model with average word2vec features is a better choice for sentiment analysis tasks than the Simple RNN model considering the first 20 words of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Considering Gated recurrent Unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the GRU architecture\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 6000 # size of word2vec features\n",
    "hidden_size = 20\n",
    "output_size = 3 # number of ratings\n",
    "\n",
    "# Defining hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1830853968858719\n",
      "Epoch [20/50], Loss: 0.04943244531750679\n",
      "Epoch [30/50], Loss: 0.03577134758234024\n",
      "Epoch [40/50], Loss: 0.031040778383612633\n",
      "Epoch [50/50], Loss: 0.025461552664637566\n"
     ]
    }
   ],
   "source": [
    "# Initializing the model, loss function, and optimizer\n",
    "model = GRU(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training network\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n",
    "        batch_y = torch.LongTensor(y_train[i:i+batch_size].values)\n",
    "        \n",
    "        batch_X = batch_X.reshape(batch_size, -1, input_size)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after every 10th epoch\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GRU: 0.5400833333333334\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "with torch.no_grad():\n",
    "    h = torch.zeros(1, len(X_test), hidden_size)\n",
    "    # Convert testing set to tensors\n",
    "    inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    inputs = inputs.reshape(12000, -1, input_size)\n",
    "    labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    # Predict classes using testing set\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted == labels).sum().item() / len(labels)\n",
    "    print('Accuracy on GRU: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Considering an LSTM unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LSTM architecture\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "input_size = 6000 # size of word2vec features\n",
    "hidden_size = 20\n",
    "output_size = 3 # number of ratings\n",
    "\n",
    "# Defining hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1725163757801056\n",
      "Epoch [20/50], Loss: 0.04603668302297592\n",
      "Epoch [30/50], Loss: 0.030680838972330093\n",
      "Epoch [40/50], Loss: 0.030271146446466446\n",
      "Epoch [50/50], Loss: 0.028017841279506683\n"
     ]
    }
   ],
   "source": [
    "# Initializing the model, loss function, and optimizer\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_X = torch.FloatTensor(X_train[i:i+batch_size])\n",
    "        batch_y = torch.LongTensor(y_train[i:i+batch_size].values)\n",
    "        \n",
    "        batch_X = batch_X.reshape(batch_size, -1, input_size)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after every 10th epoch\n",
    "    if (epoch+1)%10==0:\n",
    "        print('Epoch [{}/{}], Loss: {}'.format(epoch+1, epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on LSTM: 0.5415833333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "with torch.no_grad():\n",
    "    h = torch.zeros(1, len(X_test), hidden_size)\n",
    "    c = torch.zeros(1, len(X_test), hidden_size)\n",
    "    # Convert testing set to tensors\n",
    "    inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    inputs = inputs.reshape(len(X_test), -1, input_size)\n",
    "    labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    # Predict classes using testing set\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted == labels).sum().item() / len(labels)\n",
    "    print('Accuracy on LSTM: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.\n",
    "\n",
    "Ans. Based on the accuracy values, it appears that the LSTM model performs slightly better than the GRU and Simple RNN models in terms of accuracy. The LSTM model achieved an accuracy of 0.541, while the GRU model achieved an accuracy of 0.540 and the Simple RNN model achieved an accuracy of 0.51.\n",
    "\n",
    "It is possible that the LSTM's ability to remember longer-term dependencies within the sequence of words in each review may be helping it to better capture the sentiment expressed in each review. In this particular case, the LSTM model appears to be better suited for the sentiment analysis task, and this may be due to its ability to capture longer-term dependencies and selectively remember relevant information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
